{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nn.Conv2d?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        # calls nn.Module function\n",
    "        super(Net, self).__init__()\n",
    "        # 1 input image channel, 6 output channels, 5x5 square convolution\n",
    "        # kernel\n",
    "        #define functions\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        #affine -> y = Wx + b\n",
    "        # input is 16*5*5 vector (well its 16x5x5 image, but gets flattened?)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Max pooling over a (2,2) window\n",
    "        x = F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n",
    "        # or you can just say 2\n",
    "        x = F.max_pool2d(F.relu(self.conv2(x)), 2)\n",
    "        # flatten?\n",
    "        x = x.view(-1,self.num_flat_features(x))\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "    \n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### create the network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = Net()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### parameters randomly initialized?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 1, 5, 5])\n",
      "[-0.00510966]\n",
      "[ 0.01352612]\n",
      "[ 1.42102361]\n",
      "torch.Size([6])\n",
      "[-0.08028553]\n",
      "[ 0.01111766]\n",
      "[ 0.30702263]\n",
      "torch.Size([16, 6, 5, 5])\n",
      "[ 0.00123969]\n",
      "[ 0.00230243]\n",
      "[ 2.35100484]\n",
      "torch.Size([16])\n",
      "[ 0.00986653]\n",
      "[ 0.00289939]\n",
      "[ 0.21224613]\n",
      "torch.Size([120, 400])\n",
      "[ 0.00010932]\n",
      "[ 0.00083616]\n",
      "[ 6.33524752]\n",
      "torch.Size([120])\n",
      "[-0.00048936]\n",
      "[ 0.00085036]\n",
      "[ 0.31815371]\n",
      "torch.Size([84, 120])\n",
      "[ 0.00046801]\n",
      "[ 0.00275739]\n",
      "[ 5.27199364]\n",
      "torch.Size([84])\n",
      "[-0.0009875]\n",
      "[ 0.00272932]\n",
      "[ 0.47604105]\n",
      "torch.Size([10, 84])\n",
      "[  6.97920041e-05]\n",
      "[ 0.00405315]\n",
      "[ 1.8440702]\n",
      "torch.Size([10])\n",
      "[-0.01046631]\n",
      "[ 0.00427368]\n",
      "[ 0.19889325]\n"
     ]
    }
   ],
   "source": [
    "for p in net.parameters():\n",
    "    print p.size()\n",
    "    print p.mean().data.numpy()\n",
    "    print p.var().data.numpy()\n",
    "    print p.norm().data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net (\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear (400 -> 120)\n",
      "  (fc2): Linear (120 -> 84)\n",
      "  (fc3): Linear (84 -> 10)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You defined the forward function and the backward function is automatically defined by backward function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Learnable parameters of a model are returned by net.parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "torch.Size([6, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "params = list(net.parameters())\n",
    "print(len(params))\n",
    "print(params[0].size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input to the forward is an autograd.Variable and so is the output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the input has be a minibatch, so for a 2d image for example the Conv2D takes a 4D tensor of nSamples x nChannels x Height x Width\n",
    "\n",
    "* making a pseudo batch dimension is as simple as input.unsqueeze() to add a dummy dimension of 1 as the first dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0443  0.0938 -0.1499 -0.0734  0.0874 -0.0139  0.0066 -0.1026 -0.0442  0.0010\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# make input random inputs\n",
    "input = Variable(torch.randn(1,1,32,32))\n",
    "out = net(input)\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "zero the gradient buffers of all parameters and backprops with random gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#must specify an input gradient because the output is a vector\n",
    "#so we we can't get a gradient becasue gradient dL/dx requires that\n",
    "#L be a scalar but if L is a vector and M is scalar that is a function of L\n",
    "# then as long as we provide out with dM/dL, which should be a vector\n",
    "# of length L then, we can compute gradients because if a parameter w\n",
    "# has an affect on every element Li of L, then dM/dw = sum_i((dM/dL)_i*dLi/dw),\n",
    "out.backward(torch.ones((1,10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.1712\n",
       " 0.0667\n",
       "-0.0032\n",
       " 0.0088\n",
       " 0.0935\n",
       "-0.0895\n",
       "[torch.FloatTensor of size 6]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.conv1.bias.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recap\n",
    "* torch.Tensor\n",
    "    * multi-dimensional array\n",
    "* autograd.Variable\n",
    "    * wraps a Tensor and records history of operations applied to it\n",
    "    * has same API as Tensor, but with backward() and a few other things\n",
    "* nn.Module\n",
    "    * nice way of encapsulating parameters and has helpers for moving to a GPU, exporting, loading, etc.\n",
    "* nn.Parameter\n",
    "    * A kind of Variable that is automatically registered as a parameter when assigned as an attribute to a Module class. WOW!\n",
    "* autograd.Function\n",
    "    * implements forward and backward defintions of an autograd operation. Every Variable operation creates at least a single Function node, that connects to functions that created a variable and encodes its history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#remember input is a vector randomly initialized\n",
    "output = net(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      "-0.0443  0.0938 -0.1499 -0.0734  0.0874 -0.0139  0.0066 -0.1026 -0.0442  0.0010\n",
      "[torch.FloatTensor of size 1x10]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = Variable(torch.arange(1,11))\n",
    "criterion = nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variable containing:\n",
      " 38.7876\n",
      "[torch.FloatTensor of size 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(output, target)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.autograd.function.MSELossBackward object at 0x11749a050>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<torch.autograd.function.AddmmBackward object at 0x117483ed8>, 0)\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn.next_functions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<AccumulateGrad object at 0x114b19d50>\n"
     ]
    }
   ],
   "source": [
    "print(loss.grad_fn.next_functions[0][0].next_functions[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backprop\n",
    "to backprop the error, we just do loss.backward()\n",
    "but we must clear gradients or else gradients will be accumulated to existing gradients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net.zero_grad() #zeroes buffers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad before backward\n",
      "Variable containing:\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 6]\n",
      "\n",
      "Variable containing:\n",
      "(0 ,0 ,.,.) = \n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "\n",
      "(1 ,0 ,.,.) = \n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "\n",
      "(2 ,0 ,.,.) = \n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "\n",
      "(3 ,0 ,.,.) = \n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "\n",
      "(4 ,0 ,.,.) = \n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "\n",
      "(5 ,0 ,.,.) = \n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "  0  0  0  0  0\n",
      "[torch.FloatTensor of size 6x1x5x5]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('conv1.bias.grad before backward')\n",
    "print(net.conv1.bias.grad)\n",
    "#prints 6 zeroes because 6 biases ( 1 for each feature map)\n",
    "#they are zero because we have cleared the buffer, so no gradients\n",
    "#have been accumulated, so they all start at 0\n",
    "print(net.conv1.weight.grad)\n",
    "#gradient of weights is all zero also"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# computes dL/dw for every w, where w is all trainable parameters\n",
    "loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv1.bias.grad after backward\n",
      "Variable containing:\n",
      " 0.1673\n",
      "-0.0275\n",
      " 0.0086\n",
      "-0.0254\n",
      "-0.0884\n",
      " 0.1040\n",
      "[torch.FloatTensor of size 6]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('conv1.bias.grad after backward')\n",
    "print(net.conv1.bias.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "full list of loss functions here: http://pytorch.org/docs/nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update the weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the very simple SGD: w' = w - lr*dL/dw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# remember functions with underscore are in place\n",
    "# so this subtracts from each parameter it's gradient scaled by a learnign rate\n",
    "for f in net.parameters():\n",
    "    f.data.sub_(f.grad.data * learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### what if we want to do SGD with momentum or Nesterov or Adam, RMSProp, torch.optim has it for us!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "so let's just do the same as above but using the optim library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create optimizer\n",
    "\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(100):\n",
    "    # in your training loop:\n",
    "    optimizer.zero_grad() #zero the ol gradient\n",
    "    output = net(input) # forward prop for output\n",
    "    loss = criterion(output, target) #forward prop loss\n",
    "    loss.backward() #calculate gradients with backprop\n",
    "    optimizer.step() # update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e-04 *\n",
       "  7.5026\n",
       "[torch.FloatTensor of size 1]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
