{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import argparse\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "image_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "cifar_dataset = dset.CIFAR10(root=\"./data\", download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.Scale(image_size),\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "                       ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_dataloader = torch.utils.data.DataLoader(cifar_dataset, batch_size=batch_size,\n",
    "                                         shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ngpu = 1\n",
    "nz = 100 #dimensionality of z\n",
    "ngf = 64\n",
    "ndf = 64\n",
    "nc = 3\n",
    "netG_path = ''\n",
    "netD_path = ''\n",
    "cuda = False\n",
    "lr = 0.0002\n",
    "beta1 = 0.5\n",
    "niter = 5\n",
    "outf = \"./saved_stuff\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# custom weights initialization called on netG and netD ()\n",
    "def weights_init(m):\n",
    "    classname = m.__class__.__name__\n",
    "    if classname.find('Conv') != -1: # if conv layer then normal mean 0 std 0.02\n",
    "        #note the inplace here with the underscore (makes the weights a normal distribution)\n",
    "        m.weight.data.normal_(0.0, 0.02) #no bias for conv when we use batchnorm\n",
    "    elif classname.find('BatchNorm') != -1:\n",
    "        m.weight.data.normal_(1.0, 0.02) # if batch norm (mean 1.0, stdv 0.02)\n",
    "        m.bias.data.fill_(0) #bias 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _netG(nn.Module): #generator class\n",
    "    \"\"\"deconv network going from a nz long vector to 3x64x64 images\"\"\"\n",
    "    def __init__(self,ngpu):\n",
    "        super(_netG,self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        #define this nice seuqnetial container, so we have a nice \n",
    "        #function to apply whole network to input\n",
    "        self.main = nn.Sequential(\n",
    "            # input is nz long vector, num channgels is ngf*8, 4x4 filter, stride 1, pad 0\n",
    "            # (acts on essentially a length(z) X 1x1 image  \n",
    "            # and just does a scalar product of each kernel with the corresponding element of the vector)\n",
    "            # no bias because we are doing batch norm!\n",
    "            nn.ConvTranspose2d(nz, ngf*8,kernel_size=4,stride=1,padding=0,bias=False),\n",
    "            #pass number of channels to batch norm because it learns a scale and bias per channel\n",
    "            nn.BatchNorm2d(ngf*8),\n",
    "            #relu inplace\n",
    "            nn.ReLU(True),\n",
    "            # above: classic conv (without bias), batchnorm, then relu\n",
    "            #output is ngf*8 x 4 x 4\n",
    "            \n",
    "            \n",
    "            # now we have ngf*8 channels coming in ngf* 4 coming out, 4x4 filter, stride 2, pad 1\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            #should upsample from ngf*8 x 4 x 4 to 8x8\n",
    "            \n",
    "            #upsamples from 8x8 to 16x16\n",
    "            # 2*(8-1) + 4 - 2*1\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            \n",
    "            # state size. (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2,     ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            \n",
    "            # state size. (ngf) x 32 x 32\n",
    "            nn.ConvTranspose2d(    ngf,      nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh() #dcgan uses tanh\n",
    "            # state size. (nc) x 64 x 64\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "        # if more than one gpu then define forward pass as a data parallel option\n",
    "        if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "            output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "        else:\n",
    "            output = self.main(input)\n",
    "        return output\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netG (\n",
      "  (main): Sequential (\n",
      "    (0): ConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (2): ReLU (inplace)\n",
      "    (3): ConvTranspose2d(512, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (5): ReLU (inplace)\n",
      "    (6): ConvTranspose2d(256, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (7): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (8): ReLU (inplace)\n",
      "    (9): ConvTranspose2d(128, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (10): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (11): ReLU (inplace)\n",
      "    (12): ConvTranspose2d(64, 3, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (13): Tanh ()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netG = _netG(ngpu) # make network instance\n",
    "\n",
    "# the apply(fn) function basically goes thru every submodule in your network\n",
    "# class (each layer basically) and applys fn to every module\n",
    "netG.apply(weights_init) #initiallize weights\n",
    "if netG_path != '':\n",
    "    netG.load_state_dict(torch.load(netG_path))\n",
    "print(netG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make discriminator\n",
    "class _netD(nn.Module):\n",
    "    def __init__(self, ngpu):\n",
    "        super(_netD, self).__init__()\n",
    "        self.ngpu = ngpu\n",
    "        self.main = nn.Sequential(\n",
    "            # input is (nc) x 64 x 64\n",
    "            # no bias because batchnorm\n",
    "            nn.Conv2d(nc, ndf, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True), #ahh leakyrelu insteresting\n",
    "            # state size. (ndf) x 32 x 32\n",
    "            nn.Conv2d(ndf, ndf * 2, kernel_size=4, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*2) x 16 x 16\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*4) x 8 x 8\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            # state size. (ndf*8) x 4 x 4\n",
    "            nn.Conv2d(ndf * 8, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
    "            #output is 1x1x1 -> a scalar to be passed to sigmoid\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "    def forward(self, input):\n",
    "            if isinstance(input.data, torch.cuda.FloatTensor) and self.ngpu > 1:\n",
    "                output = nn.parallel.data_parallel(self.main, input, range(self.ngpu))\n",
    "            else:\n",
    "                output = self.main(input)\n",
    "            \n",
    "            # flatten the nx1x1x1 to be nx1 then squeeze the 1th dimension so its just a (n,) tensor\n",
    "            return output.view(-1, 1).squeeze(1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "netD = _netD(ngpu) #make instance of discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_netD (\n",
      "  (main): Sequential (\n",
      "    (0): Conv2d(3, 64, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (1): LeakyReLU (0.2, inplace)\n",
      "    (2): Conv2d(64, 128, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (3): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (4): LeakyReLU (0.2, inplace)\n",
      "    (5): Conv2d(128, 256, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (6): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (7): LeakyReLU (0.2, inplace)\n",
      "    (8): Conv2d(256, 512, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "    (9): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (10): LeakyReLU (0.2, inplace)\n",
      "    (11): Conv2d(512, 1, kernel_size=(4, 4), stride=(1, 1), bias=False)\n",
      "    (12): Sigmoid ()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "netD.apply(weights_init)\n",
    "if netD_path != '':\n",
    "    netD.load_state_dict(torch.load(netD_path))\n",
    "print(netD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the loss (in this case its binary cross entropy) cuz its \n",
    "# real vs fake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#makes a 4D tensor batch x 3 (rgb)x 64x64\n",
    "input = torch.FloatTensor(batch_size, 3, image_size, image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# makes a batch x nz x 1 x 1 4D tensor for the vector of noise\n",
    "noise = torch.FloatTensor(batch_size, nz, 1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# makes vector or guassian noise\n",
    "fixed_noise = torch.FloatTensor(batch_size, nz, 1,1).normal_(0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# makes 1d label (cuz its just a binary label)\n",
    "label = torch.FloatTensor(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "real_label = 1\n",
    "fake_label = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "if cuda:\n",
    "    #transfer all weights, tensors to gpu\n",
    "    netD.cuda()\n",
    "    netG.cuda()\n",
    "    criterion.cuda()\n",
    "    input, label = input.cuda(), label.cuda()\n",
    "    noise, fixed_noise = noise.cuda(), fixed_noise.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fixed_noise = Variable(fixed_noise)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# set up the optimizers\n",
    "# they basically take in the parameters you want to optimize and hyperparameters\n",
    "# for that optimizer\n",
    "# we have a separate optimizer for generator and discriminator "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerD = optim.Adam(netD.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizerG = optim.Adam(netG.parameters(), lr=lr, betas=(beta1, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ok we setup criterion and optimizers, we're set to go with the \n",
    "# training loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][0/3125] Loss_D: 1.9055 Loss_G: 3.5048 D(x): 0.3053 D(G(z)): 0.2774 / 0.0357\n",
      "[0/5][1/3125] Loss_D: 1.9156 Loss_G: 6.4784 D(x): 0.7515 D(G(z)): 0.7580 / 0.0023\n",
      "[0/5][2/3125] Loss_D: 0.9732 Loss_G: 6.9638 D(x): 0.7230 D(G(z)): 0.4143 / 0.0011\n",
      "[0/5][3/3125] Loss_D: 0.9062 Loss_G: 6.0972 D(x): 0.6594 D(G(z)): 0.2803 / 0.0028\n",
      "[0/5][4/3125] Loss_D: 0.6654 Loss_G: 5.0112 D(x): 0.7024 D(G(z)): 0.1618 / 0.0078\n",
      "[0/5][5/3125] Loss_D: 1.2282 Loss_G: 6.9759 D(x): 0.8486 D(G(z)): 0.4697 / 0.0012\n",
      "[0/5][6/3125] Loss_D: 0.6821 Loss_G: 7.3622 D(x): 0.7481 D(G(z)): 0.2871 / 0.0008\n",
      "[0/5][7/3125] Loss_D: 0.6123 Loss_G: 6.8587 D(x): 0.7689 D(G(z)): 0.1970 / 0.0011\n",
      "[0/5][8/3125] Loss_D: 0.9019 Loss_G: 7.7996 D(x): 0.7356 D(G(z)): 0.3559 / 0.0005\n",
      "[0/5][9/3125] Loss_D: 1.0170 Loss_G: 10.0554 D(x): 0.8377 D(G(z)): 0.4637 / 0.0001\n",
      "[0/5][10/3125] Loss_D: 1.0367 Loss_G: 7.4316 D(x): 0.5978 D(G(z)): 0.1544 / 0.0007\n",
      "[0/5][11/3125] Loss_D: 0.6593 Loss_G: 9.6785 D(x): 0.9008 D(G(z)): 0.3906 / 0.0001\n",
      "[0/5][12/3125] Loss_D: 0.7419 Loss_G: 8.0960 D(x): 0.7681 D(G(z)): 0.1468 / 0.0004\n",
      "[0/5][13/3125] Loss_D: 0.6818 Loss_G: 8.1285 D(x): 0.7336 D(G(z)): 0.2398 / 0.0003\n",
      "[0/5][14/3125] Loss_D: 0.7540 Loss_G: 10.6509 D(x): 0.8235 D(G(z)): 0.3729 / 0.0000\n",
      "[0/5][15/3125] Loss_D: 0.6035 Loss_G: 7.0472 D(x): 0.6631 D(G(z)): 0.0479 / 0.0011\n",
      "[0/5][16/3125] Loss_D: 1.3216 Loss_G: 10.0812 D(x): 0.6661 D(G(z)): 0.4107 / 0.0001\n",
      "[0/5][17/3125] Loss_D: 0.4607 Loss_G: 10.0560 D(x): 0.8546 D(G(z)): 0.1994 / 0.0001\n",
      "[0/5][18/3125] Loss_D: 0.6972 Loss_G: 8.4680 D(x): 0.7268 D(G(z)): 0.1532 / 0.0004\n",
      "[0/5][19/3125] Loss_D: 0.8657 Loss_G: 14.1062 D(x): 0.9808 D(G(z)): 0.5088 / 0.0000\n",
      "[0/5][20/3125] Loss_D: 0.1003 Loss_G: 10.9971 D(x): 0.9276 D(G(z)): 0.0113 / 0.0000\n",
      "[0/5][21/3125] Loss_D: 0.4293 Loss_G: 7.0908 D(x): 0.8055 D(G(z)): 0.0655 / 0.0012\n",
      "[0/5][22/3125] Loss_D: 1.9223 Loss_G: 16.5476 D(x): 0.8169 D(G(z)): 0.7660 / 0.0000\n",
      "[0/5][23/3125] Loss_D: 0.6388 Loss_G: 14.8850 D(x): 0.7223 D(G(z)): 0.0003 / 0.0000\n",
      "[0/5][24/3125] Loss_D: 0.3424 Loss_G: 10.3892 D(x): 0.7956 D(G(z)): 0.0006 / 0.0001\n",
      "[0/5][25/3125] Loss_D: 0.2994 Loss_G: 6.4749 D(x): 0.8814 D(G(z)): 0.1322 / 0.0017\n",
      "[0/5][26/3125] Loss_D: 2.0730 Loss_G: 17.6568 D(x): 0.9852 D(G(z)): 0.7956 / 0.0000\n",
      "[0/5][27/3125] Loss_D: 0.2984 Loss_G: 16.5840 D(x): 0.7911 D(G(z)): 0.0002 / 0.0000\n",
      "[0/5][28/3125] Loss_D: 0.2939 Loss_G: 10.9346 D(x): 0.7945 D(G(z)): 0.0009 / 0.0001\n",
      "[0/5][29/3125] Loss_D: 0.7533 Loss_G: 7.8943 D(x): 0.7261 D(G(z)): 0.2187 / 0.0004\n",
      "[0/5][30/3125] Loss_D: 1.6685 Loss_G: 18.4198 D(x): 0.9026 D(G(z)): 0.7164 / 0.0000\n",
      "[0/5][31/3125] Loss_D: 0.4549 Loss_G: 17.7459 D(x): 0.7493 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][32/3125] Loss_D: 0.3677 Loss_G: 13.2126 D(x): 0.7696 D(G(z)): 0.0001 / 0.0000\n",
      "[0/5][33/3125] Loss_D: 0.0748 Loss_G: 6.3397 D(x): 0.9516 D(G(z)): 0.0147 / 0.0038\n",
      "[0/5][34/3125] Loss_D: 1.9699 Loss_G: 18.2786 D(x): 0.8848 D(G(z)): 0.7817 / 0.0000\n",
      "[0/5][35/3125] Loss_D: 0.8664 Loss_G: 17.3106 D(x): 0.7161 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][36/3125] Loss_D: 0.2192 Loss_G: 13.0067 D(x): 0.8598 D(G(z)): 0.0001 / 0.0000\n",
      "[0/5][37/3125] Loss_D: 0.2162 Loss_G: 6.4156 D(x): 0.8474 D(G(z)): 0.0155 / 0.0051\n",
      "[0/5][38/3125] Loss_D: 2.4256 Loss_G: 19.2446 D(x): 0.9026 D(G(z)): 0.8611 / 0.0000\n",
      "[0/5][39/3125] Loss_D: 0.2488 Loss_G: 19.7134 D(x): 0.8653 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][40/3125] Loss_D: 0.3320 Loss_G: 16.8400 D(x): 0.7830 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][41/3125] Loss_D: 0.3088 Loss_G: 9.9080 D(x): 0.8052 D(G(z)): 0.0002 / 0.0001\n",
      "[0/5][42/3125] Loss_D: 0.3974 Loss_G: 5.9437 D(x): 0.8481 D(G(z)): 0.1216 / 0.0032\n",
      "[0/5][43/3125] Loss_D: 2.2910 Loss_G: 20.3343 D(x): 0.9266 D(G(z)): 0.8386 / 0.0000\n",
      "[0/5][44/3125] Loss_D: 0.2027 Loss_G: 22.5710 D(x): 0.8568 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][45/3125] Loss_D: 0.4862 Loss_G: 21.1820 D(x): 0.8352 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][46/3125] Loss_D: 0.2843 Loss_G: 16.1754 D(x): 0.8186 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][47/3125] Loss_D: 0.0202 Loss_G: 9.5346 D(x): 0.9808 D(G(z)): 0.0002 / 0.0002\n",
      "[0/5][48/3125] Loss_D: 0.1548 Loss_G: 5.6864 D(x): 0.9515 D(G(z)): 0.0838 / 0.0045\n",
      "[0/5][49/3125] Loss_D: 1.2987 Loss_G: 19.5675 D(x): 0.9819 D(G(z)): 0.6739 / 0.0000\n",
      "[0/5][50/3125] Loss_D: 0.3109 Loss_G: 20.9899 D(x): 0.8030 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][51/3125] Loss_D: 0.2781 Loss_G: 18.1145 D(x): 0.8174 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][52/3125] Loss_D: 0.1767 Loss_G: 12.4602 D(x): 0.8608 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][53/3125] Loss_D: 0.0586 Loss_G: 6.2334 D(x): 0.9519 D(G(z)): 0.0043 / 0.0031\n",
      "[0/5][54/3125] Loss_D: 1.1242 Loss_G: 17.8981 D(x): 0.9726 D(G(z)): 0.6115 / 0.0000\n",
      "[0/5][55/3125] Loss_D: 0.0619 Loss_G: 19.5770 D(x): 0.9426 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][56/3125] Loss_D: 0.2244 Loss_G: 17.8274 D(x): 0.8493 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][57/3125] Loss_D: 0.1817 Loss_G: 12.4044 D(x): 0.8599 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][58/3125] Loss_D: 0.0878 Loss_G: 5.3745 D(x): 0.9301 D(G(z)): 0.0101 / 0.0067\n",
      "[0/5][59/3125] Loss_D: 1.9520 Loss_G: 21.8977 D(x): 0.8639 D(G(z)): 0.7645 / 0.0000\n",
      "[0/5][60/3125] Loss_D: 0.3583 Loss_G: 24.4484 D(x): 0.7678 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][61/3125] Loss_D: 0.2056 Loss_G: 23.1362 D(x): 0.8470 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][62/3125] Loss_D: 0.1346 Loss_G: 18.7337 D(x): 0.8951 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][63/3125] Loss_D: 0.0809 Loss_G: 13.0352 D(x): 0.9382 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][64/3125] Loss_D: 0.0253 Loss_G: 6.1688 D(x): 0.9822 D(G(z)): 0.0059 / 0.0048\n",
      "[0/5][65/3125] Loss_D: 1.3244 Loss_G: 20.3168 D(x): 0.9914 D(G(z)): 0.6704 / 0.0000\n",
      "[0/5][66/3125] Loss_D: 0.2794 Loss_G: 22.7621 D(x): 0.8912 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][67/3125] Loss_D: 0.0639 Loss_G: 20.8844 D(x): 0.9422 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][68/3125] Loss_D: 0.0369 Loss_G: 15.8484 D(x): 0.9659 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][69/3125] Loss_D: 0.0887 Loss_G: 8.3489 D(x): 0.9188 D(G(z)): 0.0011 / 0.0009\n",
      "[0/5][70/3125] Loss_D: 1.2280 Loss_G: 18.7903 D(x): 0.8129 D(G(z)): 0.5802 / 0.0000\n",
      "[0/5][71/3125] Loss_D: 0.2827 Loss_G: 19.6354 D(x): 0.8362 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][72/3125] Loss_D: 0.4116 Loss_G: 15.6817 D(x): 0.7428 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][73/3125] Loss_D: 0.0866 Loss_G: 8.9891 D(x): 0.9238 D(G(z)): 0.0002 / 0.0002\n",
      "[0/5][74/3125] Loss_D: 0.2666 Loss_G: 8.8542 D(x): 0.9654 D(G(z)): 0.1927 / 0.0002\n",
      "[0/5][75/3125] Loss_D: 0.2828 Loss_G: 9.3660 D(x): 0.8663 D(G(z)): 0.1016 / 0.0001\n",
      "[0/5][76/3125] Loss_D: 0.3727 Loss_G: 7.2485 D(x): 0.8165 D(G(z)): 0.0717 / 0.0009\n",
      "[0/5][77/3125] Loss_D: 0.4478 Loss_G: 13.0882 D(x): 0.8901 D(G(z)): 0.2456 / 0.0000\n",
      "[0/5][78/3125] Loss_D: 0.2480 Loss_G: 11.0367 D(x): 0.8672 D(G(z)): 0.0009 / 0.0000\n",
      "[0/5][79/3125] Loss_D: 0.3236 Loss_G: 4.9263 D(x): 0.7966 D(G(z)): 0.0070 / 0.0083\n",
      "[0/5][80/3125] Loss_D: 1.5120 Loss_G: 23.6310 D(x): 0.9859 D(G(z)): 0.7297 / 0.0000\n",
      "[0/5][81/3125] Loss_D: 1.1859 Loss_G: 25.5783 D(x): 0.5997 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][82/3125] Loss_D: 0.3646 Loss_G: 23.2079 D(x): 0.8208 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][83/3125] Loss_D: 0.1291 Loss_G: 19.1236 D(x): 0.9038 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][84/3125] Loss_D: 0.0064 Loss_G: 13.5581 D(x): 0.9937 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][85/3125] Loss_D: 0.0650 Loss_G: 7.8413 D(x): 0.9495 D(G(z)): 0.0003 / 0.0006\n",
      "[0/5][86/3125] Loss_D: 0.0970 Loss_G: 5.1687 D(x): 0.9804 D(G(z)): 0.0724 / 0.0075\n",
      "[0/5][87/3125] Loss_D: 0.5601 Loss_G: 14.9525 D(x): 0.9466 D(G(z)): 0.3746 / 0.0000\n",
      "[0/5][88/3125] Loss_D: 0.2576 Loss_G: 15.9089 D(x): 0.8332 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][89/3125] Loss_D: 0.0580 Loss_G: 14.5453 D(x): 0.9488 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][90/3125] Loss_D: 0.1570 Loss_G: 10.3490 D(x): 0.8899 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][91/3125] Loss_D: 0.1058 Loss_G: 4.8934 D(x): 0.9275 D(G(z)): 0.0024 / 0.0081\n",
      "[0/5][92/3125] Loss_D: 0.5982 Loss_G: 15.7501 D(x): 0.9966 D(G(z)): 0.4264 / 0.0000\n",
      "[0/5][93/3125] Loss_D: 0.0257 Loss_G: 18.7813 D(x): 0.9752 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][94/3125] Loss_D: 0.9037 Loss_G: 14.0637 D(x): 0.5921 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][95/3125] Loss_D: 0.0159 Loss_G: 9.7121 D(x): 0.9845 D(G(z)): 0.0001 / 0.0001\n",
      "[0/5][96/3125] Loss_D: 0.0677 Loss_G: 4.7814 D(x): 0.9448 D(G(z)): 0.0082 / 0.0119\n",
      "[0/5][97/3125] Loss_D: 0.7643 Loss_G: 17.9567 D(x): 0.9857 D(G(z)): 0.5100 / 0.0000\n",
      "[0/5][98/3125] Loss_D: 0.0724 Loss_G: 21.7850 D(x): 0.9361 D(G(z)): 0.0000 / 0.0000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][99/3125] Loss_D: 0.4525 Loss_G: 18.6216 D(x): 0.7803 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][100/3125] Loss_D: 0.0740 Loss_G: 15.3031 D(x): 0.9361 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][101/3125] Loss_D: 0.0487 Loss_G: 9.5065 D(x): 0.9555 D(G(z)): 0.0001 / 0.0001\n",
      "[0/5][102/3125] Loss_D: 0.0366 Loss_G: 4.5901 D(x): 0.9825 D(G(z)): 0.0177 / 0.0125\n",
      "[0/5][103/3125] Loss_D: 1.8399 Loss_G: 25.5151 D(x): 0.9058 D(G(z)): 0.7836 / 0.0000\n",
      "[0/5][104/3125] Loss_D: 0.0703 Loss_G: 27.3194 D(x): 0.9336 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][105/3125] Loss_D: 1.5343 Loss_G: 27.4917 D(x): 0.5434 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][106/3125] Loss_D: 0.1239 Loss_G: 26.7023 D(x): 0.9057 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][107/3125] Loss_D: 0.3370 Loss_G: 26.7923 D(x): 0.7975 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][108/3125] Loss_D: 0.1020 Loss_G: 23.6624 D(x): 0.9394 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][109/3125] Loss_D: 0.0397 Loss_G: 20.0154 D(x): 0.9637 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][110/3125] Loss_D: 0.0041 Loss_G: 13.3327 D(x): 0.9959 D(G(z)): 0.0000 / 0.0001\n",
      "[0/5][111/3125] Loss_D: 0.0620 Loss_G: 6.6915 D(x): 0.9960 D(G(z)): 0.0475 / 0.0061\n",
      "[0/5][112/3125] Loss_D: 1.1837 Loss_G: 23.2883 D(x): 0.9975 D(G(z)): 0.5975 / 0.0000\n",
      "[0/5][113/3125] Loss_D: 0.0448 Loss_G: 26.0658 D(x): 0.9585 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][114/3125] Loss_D: 0.8335 Loss_G: 25.0295 D(x): 0.6554 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][115/3125] Loss_D: 0.4256 Loss_G: 21.3095 D(x): 0.8136 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][116/3125] Loss_D: 0.1675 Loss_G: 15.6386 D(x): 0.8923 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][117/3125] Loss_D: 0.0080 Loss_G: 7.7591 D(x): 0.9925 D(G(z)): 0.0004 / 0.0008\n",
      "[0/5][118/3125] Loss_D: 1.6360 Loss_G: 23.6371 D(x): 0.9722 D(G(z)): 0.7238 / 0.0000\n",
      "[0/5][119/3125] Loss_D: 0.5827 Loss_G: 25.8704 D(x): 0.8272 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][120/3125] Loss_D: 0.5558 Loss_G: 24.4231 D(x): 0.7585 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][121/3125] Loss_D: 0.0835 Loss_G: 18.2256 D(x): 0.9430 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][122/3125] Loss_D: 0.1196 Loss_G: 7.6591 D(x): 0.9305 D(G(z)): 0.0011 / 0.0009\n",
      "[0/5][123/3125] Loss_D: 3.3092 Loss_G: 26.6194 D(x): 0.8620 D(G(z)): 0.9222 / 0.0000\n",
      "[0/5][124/3125] Loss_D: 0.6512 Loss_G: 27.5228 D(x): 0.7866 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][125/3125] Loss_D: 1.2412 Loss_G: 27.0811 D(x): 0.5329 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][126/3125] Loss_D: 0.0283 Loss_G: 27.1274 D(x): 0.9740 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][127/3125] Loss_D: 0.0882 Loss_G: 26.6199 D(x): 0.9319 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][128/3125] Loss_D: 0.0768 Loss_G: 25.2389 D(x): 0.9500 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][129/3125] Loss_D: 0.0044 Loss_G: 22.4639 D(x): 0.9956 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][130/3125] Loss_D: 0.0172 Loss_G: 14.8410 D(x): 0.9833 D(G(z)): 0.0000 / 0.0000\n",
      "[0/5][131/3125] Loss_D: 0.0162 Loss_G: 6.7920 D(x): 0.9871 D(G(z)): 0.0028 / 0.0031\n",
      "[0/5][132/3125] Loss_D: 0.3187 Loss_G: 10.5045 D(x): 0.9967 D(G(z)): 0.2564 / 0.0000\n",
      "[0/5][133/3125] Loss_D: 0.0438 Loss_G: 10.9099 D(x): 0.9664 D(G(z)): 0.0017 / 0.0000\n",
      "[0/5][134/3125] Loss_D: 0.0095 Loss_G: 8.9212 D(x): 0.9919 D(G(z)): 0.0012 / 0.0002\n",
      "[0/5][135/3125] Loss_D: 0.0689 Loss_G: 5.6694 D(x): 0.9556 D(G(z)): 0.0126 / 0.0049\n",
      "[0/5][136/3125] Loss_D: 0.1208 Loss_G: 6.2201 D(x): 0.9779 D(G(z)): 0.0915 / 0.0027\n",
      "[0/5][137/3125] Loss_D: 0.1296 Loss_G: 7.8130 D(x): 0.9916 D(G(z)): 0.1092 / 0.0007\n",
      "[0/5][138/3125] Loss_D: 0.1413 Loss_G: 6.9317 D(x): 0.9110 D(G(z)): 0.0072 / 0.0016\n",
      "[0/5][139/3125] Loss_D: 0.4297 Loss_G: 4.7671 D(x): 0.8425 D(G(z)): 0.1170 / 0.0107\n",
      "[0/5][140/3125] Loss_D: 0.5953 Loss_G: 5.9180 D(x): 0.7940 D(G(z)): 0.1631 / 0.0036\n",
      "[0/5][141/3125] Loss_D: 0.1110 Loss_G: 7.0101 D(x): 0.9685 D(G(z)): 0.0707 / 0.0013\n",
      "[0/5][142/3125] Loss_D: 0.3454 Loss_G: 5.5514 D(x): 0.8560 D(G(z)): 0.0675 / 0.0068\n",
      "[0/5][143/3125] Loss_D: 0.3447 Loss_G: 9.7421 D(x): 0.9939 D(G(z)): 0.2313 / 0.0001\n",
      "[0/5][144/3125] Loss_D: 0.0215 Loss_G: 10.1236 D(x): 0.9817 D(G(z)): 0.0023 / 0.0001\n",
      "[0/5][145/3125] Loss_D: 0.3337 Loss_G: 5.8358 D(x): 0.8008 D(G(z)): 0.0037 / 0.0056\n",
      "[0/5][146/3125] Loss_D: 0.0853 Loss_G: 4.6609 D(x): 0.9527 D(G(z)): 0.0317 / 0.0166\n",
      "[0/5][147/3125] Loss_D: 0.6482 Loss_G: 10.7542 D(x): 0.8973 D(G(z)): 0.3717 / 0.0000\n",
      "[0/5][148/3125] Loss_D: 0.3514 Loss_G: 8.4990 D(x): 0.8660 D(G(z)): 0.0036 / 0.0008\n",
      "[0/5][149/3125] Loss_D: 0.2639 Loss_G: 5.9767 D(x): 0.8375 D(G(z)): 0.0299 / 0.0232\n",
      "[0/5][150/3125] Loss_D: 0.5928 Loss_G: 7.3580 D(x): 0.8632 D(G(z)): 0.2648 / 0.0010\n",
      "[0/5][151/3125] Loss_D: 0.9924 Loss_G: 5.1287 D(x): 0.6540 D(G(z)): 0.1133 / 0.0085\n",
      "[0/5][152/3125] Loss_D: 0.1313 Loss_G: 5.3583 D(x): 0.9540 D(G(z)): 0.0671 / 0.0075\n",
      "[0/5][153/3125] Loss_D: 0.2766 Loss_G: 6.8905 D(x): 0.9742 D(G(z)): 0.2101 / 0.0017\n",
      "[0/5][154/3125] Loss_D: 0.1048 Loss_G: 6.9206 D(x): 0.9512 D(G(z)): 0.0418 / 0.0025\n",
      "[0/5][155/3125] Loss_D: 0.2260 Loss_G: 4.5697 D(x): 0.8509 D(G(z)): 0.0369 / 0.0160\n",
      "[0/5][156/3125] Loss_D: 0.2986 Loss_G: 6.3493 D(x): 0.9961 D(G(z)): 0.2291 / 0.0030\n",
      "[0/5][157/3125] Loss_D: 0.6420 Loss_G: 4.7479 D(x): 0.6825 D(G(z)): 0.0517 / 0.0234\n",
      "[0/5][158/3125] Loss_D: 0.3125 Loss_G: 6.8338 D(x): 0.9991 D(G(z)): 0.2398 / 0.0017\n",
      "[0/5][159/3125] Loss_D: 0.0819 Loss_G: 6.9402 D(x): 0.9398 D(G(z)): 0.0160 / 0.0018\n",
      "[0/5][160/3125] Loss_D: 0.1457 Loss_G: 5.3822 D(x): 0.9533 D(G(z)): 0.0842 / 0.0075\n",
      "[0/5][161/3125] Loss_D: 0.2820 Loss_G: 5.9499 D(x): 0.9045 D(G(z)): 0.1392 / 0.0044\n",
      "[0/5][162/3125] Loss_D: 0.3581 Loss_G: 5.0530 D(x): 0.8704 D(G(z)): 0.0479 / 0.0128\n",
      "[0/5][163/3125] Loss_D: 0.2450 Loss_G: 3.4813 D(x): 0.8507 D(G(z)): 0.0456 / 0.0445\n",
      "[0/5][164/3125] Loss_D: 1.2597 Loss_G: 14.4305 D(x): 0.9418 D(G(z)): 0.6476 / 0.0000\n",
      "[0/5][165/3125] Loss_D: 2.3037 Loss_G: 10.3558 D(x): 0.2883 D(G(z)): 0.0000 / 0.0001\n",
      "[0/5][166/3125] Loss_D: 0.2785 Loss_G: 6.7257 D(x): 0.8069 D(G(z)): 0.0010 / 0.0132\n",
      "[0/5][167/3125] Loss_D: 0.2714 Loss_G: 4.2312 D(x): 0.9198 D(G(z)): 0.1328 / 0.0261\n",
      "[0/5][168/3125] Loss_D: 0.1058 Loss_G: 4.8994 D(x): 0.9937 D(G(z)): 0.0908 / 0.0135\n",
      "[0/5][169/3125] Loss_D: 0.4614 Loss_G: 5.7087 D(x): 0.8572 D(G(z)): 0.2034 / 0.0050\n",
      "[0/5][170/3125] Loss_D: 0.2117 Loss_G: 6.7485 D(x): 0.9872 D(G(z)): 0.1660 / 0.0020\n",
      "[0/5][171/3125] Loss_D: 0.3869 Loss_G: 5.7946 D(x): 0.7647 D(G(z)): 0.0126 / 0.0092\n",
      "[0/5][172/3125] Loss_D: 0.2643 Loss_G: 4.2632 D(x): 0.9249 D(G(z)): 0.1469 / 0.0310\n",
      "[0/5][173/3125] Loss_D: 0.2703 Loss_G: 5.3733 D(x): 0.9240 D(G(z)): 0.1538 / 0.0091\n",
      "[0/5][174/3125] Loss_D: 0.2389 Loss_G: 5.2609 D(x): 0.9167 D(G(z)): 0.1118 / 0.0084\n",
      "[0/5][175/3125] Loss_D: 0.2735 Loss_G: 4.8997 D(x): 0.9168 D(G(z)): 0.0962 / 0.0094\n",
      "[0/5][176/3125] Loss_D: 0.2795 Loss_G: 5.7363 D(x): 0.9106 D(G(z)): 0.1562 / 0.0045\n",
      "[0/5][177/3125] Loss_D: 0.9310 Loss_G: 3.7065 D(x): 0.6377 D(G(z)): 0.1741 / 0.0333\n",
      "[0/5][178/3125] Loss_D: 0.5917 Loss_G: 5.4346 D(x): 0.7870 D(G(z)): 0.2260 / 0.0074\n",
      "[0/5][179/3125] Loss_D: 0.5209 Loss_G: 7.2225 D(x): 0.9055 D(G(z)): 0.2604 / 0.0016\n",
      "[0/5][180/3125] Loss_D: 0.8997 Loss_G: 3.1191 D(x): 0.5562 D(G(z)): 0.0489 / 0.0776\n",
      "[0/5][181/3125] Loss_D: 1.0179 Loss_G: 9.6727 D(x): 0.9313 D(G(z)): 0.4686 / 0.0002\n",
      "[0/5][182/3125] Loss_D: 0.2349 Loss_G: 8.3081 D(x): 0.8277 D(G(z)): 0.0054 / 0.0010\n",
      "[0/5][183/3125] Loss_D: 0.2552 Loss_G: 6.2194 D(x): 0.8928 D(G(z)): 0.0307 / 0.0179\n",
      "[0/5][184/3125] Loss_D: 0.4559 Loss_G: 5.9251 D(x): 0.8870 D(G(z)): 0.2330 / 0.0074\n",
      "[0/5][185/3125] Loss_D: 0.6789 Loss_G: 6.3777 D(x): 0.7977 D(G(z)): 0.2013 / 0.0040\n",
      "[0/5][186/3125] Loss_D: 0.6960 Loss_G: 2.8669 D(x): 0.6481 D(G(z)): 0.0343 / 0.0872\n",
      "[0/5][187/3125] Loss_D: 1.1940 Loss_G: 8.5845 D(x): 0.9502 D(G(z)): 0.6258 / 0.0002\n",
      "[0/5][188/3125] Loss_D: 0.8952 Loss_G: 7.4118 D(x): 0.5369 D(G(z)): 0.0012 / 0.0019\n",
      "[0/5][189/3125] Loss_D: 0.0818 Loss_G: 5.6184 D(x): 0.9320 D(G(z)): 0.0068 / 0.0118\n",
      "[0/5][190/3125] Loss_D: 0.1949 Loss_G: 5.0500 D(x): 0.9853 D(G(z)): 0.1495 / 0.0130\n",
      "[0/5][191/3125] Loss_D: 0.1156 Loss_G: 5.4075 D(x): 0.9663 D(G(z)): 0.0732 / 0.0182\n",
      "[0/5][192/3125] Loss_D: 0.2965 Loss_G: 5.7232 D(x): 0.9702 D(G(z)): 0.2197 / 0.0056\n",
      "[0/5][193/3125] Loss_D: 0.3501 Loss_G: 4.3839 D(x): 0.7916 D(G(z)): 0.0766 / 0.0227\n",
      "[0/5][194/3125] Loss_D: 0.3357 Loss_G: 3.5983 D(x): 0.8413 D(G(z)): 0.1175 / 0.0373\n",
      "[0/5][195/3125] Loss_D: 0.8293 Loss_G: 9.9846 D(x): 0.9536 D(G(z)): 0.4723 / 0.0001\n",
      "[0/5][196/3125] Loss_D: 2.9441 Loss_G: 1.7951 D(x): 0.1467 D(G(z)): 0.0020 / 0.2405\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][197/3125] Loss_D: 1.2777 Loss_G: 7.3821 D(x): 0.8850 D(G(z)): 0.5818 / 0.0009\n",
      "[0/5][198/3125] Loss_D: 0.5109 Loss_G: 7.0712 D(x): 0.6784 D(G(z)): 0.0041 / 0.0019\n",
      "[0/5][199/3125] Loss_D: 0.1965 Loss_G: 3.9048 D(x): 0.8624 D(G(z)): 0.0301 / 0.0558\n",
      "[0/5][200/3125] Loss_D: 0.4576 Loss_G: 3.9760 D(x): 0.8737 D(G(z)): 0.2172 / 0.0276\n",
      "[0/5][201/3125] Loss_D: 0.3214 Loss_G: 4.1288 D(x): 0.8948 D(G(z)): 0.0647 / 0.0258\n",
      "[0/5][202/3125] Loss_D: 0.2052 Loss_G: 4.5507 D(x): 0.9654 D(G(z)): 0.1473 / 0.0158\n",
      "[0/5][203/3125] Loss_D: 0.4320 Loss_G: 4.0180 D(x): 0.8315 D(G(z)): 0.1614 / 0.0248\n",
      "[0/5][204/3125] Loss_D: 0.4461 Loss_G: 5.6096 D(x): 0.8820 D(G(z)): 0.2473 / 0.0050\n",
      "[0/5][205/3125] Loss_D: 0.9519 Loss_G: 1.7856 D(x): 0.6041 D(G(z)): 0.0471 / 0.2189\n",
      "[0/5][206/3125] Loss_D: 0.8103 Loss_G: 5.4643 D(x): 0.9366 D(G(z)): 0.4540 / 0.0073\n",
      "[0/5][207/3125] Loss_D: 0.9904 Loss_G: 3.3254 D(x): 0.5651 D(G(z)): 0.0683 / 0.0738\n",
      "[0/5][208/3125] Loss_D: 0.3038 Loss_G: 5.1105 D(x): 0.9847 D(G(z)): 0.2272 / 0.0087\n",
      "[0/5][209/3125] Loss_D: 0.3771 Loss_G: 5.6621 D(x): 0.8949 D(G(z)): 0.2052 / 0.0059\n",
      "[0/5][210/3125] Loss_D: 0.4658 Loss_G: 4.3324 D(x): 0.7477 D(G(z)): 0.0380 / 0.0379\n",
      "[0/5][211/3125] Loss_D: 1.0295 Loss_G: 3.1903 D(x): 0.6517 D(G(z)): 0.2501 / 0.0780\n",
      "[0/5][212/3125] Loss_D: 0.6124 Loss_G: 5.7358 D(x): 0.8453 D(G(z)): 0.2893 / 0.0061\n",
      "[0/5][213/3125] Loss_D: 0.5736 Loss_G: 3.5448 D(x): 0.7146 D(G(z)): 0.0404 / 0.0711\n",
      "[0/5][214/3125] Loss_D: 0.2639 Loss_G: 5.0783 D(x): 0.9711 D(G(z)): 0.1739 / 0.0119\n",
      "[0/5][215/3125] Loss_D: 0.5508 Loss_G: 6.6154 D(x): 0.9591 D(G(z)): 0.3802 / 0.0019\n",
      "[0/5][216/3125] Loss_D: 1.2024 Loss_G: 3.0875 D(x): 0.4454 D(G(z)): 0.0109 / 0.1981\n",
      "[0/5][217/3125] Loss_D: 1.4245 Loss_G: 9.7533 D(x): 0.8828 D(G(z)): 0.5606 / 0.0002\n",
      "[0/5][218/3125] Loss_D: 1.5758 Loss_G: 3.9436 D(x): 0.4105 D(G(z)): 0.0160 / 0.0470\n",
      "[0/5][219/3125] Loss_D: 0.2494 Loss_G: 4.2643 D(x): 0.9573 D(G(z)): 0.1713 / 0.0277\n",
      "[0/5][220/3125] Loss_D: 0.4127 Loss_G: 3.6005 D(x): 0.8139 D(G(z)): 0.1223 / 0.0463\n",
      "[0/5][221/3125] Loss_D: 0.5950 Loss_G: 4.0784 D(x): 0.8025 D(G(z)): 0.1893 / 0.0293\n",
      "[0/5][222/3125] Loss_D: 0.6523 Loss_G: 6.0742 D(x): 0.9411 D(G(z)): 0.2909 / 0.0074\n",
      "[0/5][223/3125] Loss_D: 0.3022 Loss_G: 5.7791 D(x): 0.8012 D(G(z)): 0.0134 / 0.0064\n",
      "[0/5][224/3125] Loss_D: 0.7402 Loss_G: 3.9394 D(x): 0.8653 D(G(z)): 0.1922 / 0.0297\n",
      "[0/5][225/3125] Loss_D: 0.4529 Loss_G: 4.2087 D(x): 0.8105 D(G(z)): 0.1622 / 0.0203\n",
      "[0/5][226/3125] Loss_D: 0.2932 Loss_G: 4.6962 D(x): 0.9015 D(G(z)): 0.1285 / 0.0170\n",
      "[0/5][227/3125] Loss_D: 0.7779 Loss_G: 2.0415 D(x): 0.6487 D(G(z)): 0.1640 / 0.1691\n",
      "[0/5][228/3125] Loss_D: 0.8455 Loss_G: 8.1399 D(x): 0.9557 D(G(z)): 0.4865 / 0.0004\n",
      "[0/5][229/3125] Loss_D: 0.7593 Loss_G: 6.0353 D(x): 0.6110 D(G(z)): 0.0091 / 0.0045\n",
      "[0/5][230/3125] Loss_D: 0.3246 Loss_G: 3.3895 D(x): 0.8240 D(G(z)): 0.0636 / 0.0547\n",
      "[0/5][231/3125] Loss_D: 0.5111 Loss_G: 6.2588 D(x): 0.9751 D(G(z)): 0.3552 / 0.0026\n",
      "[0/5][232/3125] Loss_D: 0.2405 Loss_G: 6.0753 D(x): 0.8652 D(G(z)): 0.0614 / 0.0047\n",
      "[0/5][233/3125] Loss_D: 0.4282 Loss_G: 3.5670 D(x): 0.7477 D(G(z)): 0.0605 / 0.0498\n",
      "[0/5][234/3125] Loss_D: 0.8628 Loss_G: 9.5148 D(x): 0.9144 D(G(z)): 0.4700 / 0.0002\n",
      "[0/5][235/3125] Loss_D: 0.6942 Loss_G: 6.6901 D(x): 0.5942 D(G(z)): 0.0037 / 0.0043\n",
      "[0/5][236/3125] Loss_D: 0.1948 Loss_G: 4.6366 D(x): 0.9202 D(G(z)): 0.0869 / 0.0237\n",
      "[0/5][237/3125] Loss_D: 0.6167 Loss_G: 7.7845 D(x): 0.9670 D(G(z)): 0.3427 / 0.0016\n",
      "[0/5][238/3125] Loss_D: 0.5999 Loss_G: 3.3867 D(x): 0.6598 D(G(z)): 0.0515 / 0.0515\n",
      "[0/5][239/3125] Loss_D: 0.6599 Loss_G: 8.3543 D(x): 0.9509 D(G(z)): 0.4013 / 0.0006\n",
      "[0/5][240/3125] Loss_D: 0.5784 Loss_G: 6.0240 D(x): 0.6651 D(G(z)): 0.0160 / 0.0204\n",
      "[0/5][241/3125] Loss_D: 0.5432 Loss_G: 3.9763 D(x): 0.8143 D(G(z)): 0.0779 / 0.0377\n",
      "[0/5][242/3125] Loss_D: 0.6713 Loss_G: 7.8007 D(x): 0.9616 D(G(z)): 0.4030 / 0.0014\n",
      "[0/5][243/3125] Loss_D: 0.5160 Loss_G: 6.2673 D(x): 0.7010 D(G(z)): 0.0118 / 0.0034\n",
      "[0/5][244/3125] Loss_D: 0.0645 Loss_G: 4.9774 D(x): 0.9580 D(G(z)): 0.0200 / 0.0113\n",
      "[0/5][245/3125] Loss_D: 0.1842 Loss_G: 4.3045 D(x): 0.9643 D(G(z)): 0.1303 / 0.0165\n",
      "[0/5][246/3125] Loss_D: 0.8949 Loss_G: 8.3723 D(x): 0.8300 D(G(z)): 0.4229 / 0.0006\n",
      "[0/5][247/3125] Loss_D: 0.5787 Loss_G: 6.0134 D(x): 0.7208 D(G(z)): 0.0153 / 0.0073\n",
      "[0/5][248/3125] Loss_D: 0.3104 Loss_G: 4.7029 D(x): 0.9045 D(G(z)): 0.1484 / 0.0274\n",
      "[0/5][249/3125] Loss_D: 0.5493 Loss_G: 6.7648 D(x): 0.9497 D(G(z)): 0.3452 / 0.0021\n",
      "[0/5][250/3125] Loss_D: 0.9039 Loss_G: 4.6876 D(x): 0.6599 D(G(z)): 0.0345 / 0.0203\n",
      "[0/5][251/3125] Loss_D: 0.7109 Loss_G: 5.4677 D(x): 0.8607 D(G(z)): 0.2750 / 0.0067\n",
      "[0/5][252/3125] Loss_D: 0.1793 Loss_G: 5.6175 D(x): 0.9445 D(G(z)): 0.1020 / 0.0048\n",
      "[0/5][253/3125] Loss_D: 0.4521 Loss_G: 4.0388 D(x): 0.7664 D(G(z)): 0.0413 / 0.0238\n",
      "[0/5][254/3125] Loss_D: 0.5743 Loss_G: 8.9139 D(x): 0.9250 D(G(z)): 0.3362 / 0.0003\n",
      "[0/5][255/3125] Loss_D: 0.5049 Loss_G: 5.2912 D(x): 0.6837 D(G(z)): 0.0171 / 0.0188\n",
      "[0/5][256/3125] Loss_D: 0.1524 Loss_G: 5.2491 D(x): 0.9753 D(G(z)): 0.1146 / 0.0062\n",
      "[0/5][257/3125] Loss_D: 0.3982 Loss_G: 8.5724 D(x): 0.9327 D(G(z)): 0.2417 / 0.0005\n",
      "[0/5][258/3125] Loss_D: 0.4047 Loss_G: 5.8599 D(x): 0.7879 D(G(z)): 0.0061 / 0.0051\n",
      "[0/5][259/3125] Loss_D: 0.2058 Loss_G: 4.7833 D(x): 0.9304 D(G(z)): 0.1163 / 0.0147\n",
      "[0/5][260/3125] Loss_D: 0.5680 Loss_G: 9.4166 D(x): 0.9390 D(G(z)): 0.3440 / 0.0001\n",
      "[0/5][261/3125] Loss_D: 1.7552 Loss_G: 2.7810 D(x): 0.3563 D(G(z)): 0.0038 / 0.1066\n",
      "[0/5][262/3125] Loss_D: 0.8705 Loss_G: 9.3922 D(x): 0.9743 D(G(z)): 0.5225 / 0.0002\n",
      "[0/5][263/3125] Loss_D: 0.6011 Loss_G: 6.9024 D(x): 0.6249 D(G(z)): 0.0037 / 0.0020\n",
      "[0/5][264/3125] Loss_D: 0.1342 Loss_G: 4.7909 D(x): 0.9243 D(G(z)): 0.0322 / 0.0260\n",
      "[0/5][265/3125] Loss_D: 0.3184 Loss_G: 6.9391 D(x): 0.9815 D(G(z)): 0.2209 / 0.0018\n",
      "[0/5][266/3125] Loss_D: 0.2366 Loss_G: 5.9862 D(x): 0.8650 D(G(z)): 0.0483 / 0.0066\n",
      "[0/5][267/3125] Loss_D: 0.1865 Loss_G: 4.8098 D(x): 0.9053 D(G(z)): 0.0620 / 0.0170\n",
      "[0/5][268/3125] Loss_D: 0.7759 Loss_G: 11.4051 D(x): 0.9875 D(G(z)): 0.4683 / 0.0000\n",
      "[0/5][269/3125] Loss_D: 1.1275 Loss_G: 8.5921 D(x): 0.5479 D(G(z)): 0.0004 / 0.0005\n",
      "[0/5][270/3125] Loss_D: 0.0181 Loss_G: 5.9323 D(x): 0.9890 D(G(z)): 0.0069 / 0.0097\n",
      "[0/5][271/3125] Loss_D: 0.1945 Loss_G: 6.5747 D(x): 0.9689 D(G(z)): 0.1167 / 0.0032\n",
      "[0/5][272/3125] Loss_D: 0.1741 Loss_G: 5.9152 D(x): 0.9457 D(G(z)): 0.0890 / 0.0053\n",
      "[0/5][273/3125] Loss_D: 0.4179 Loss_G: 6.7154 D(x): 0.8766 D(G(z)): 0.1924 / 0.0020\n",
      "[0/5][274/3125] Loss_D: 0.3598 Loss_G: 5.2842 D(x): 0.7840 D(G(z)): 0.0401 / 0.0182\n",
      "[0/5][275/3125] Loss_D: 0.1320 Loss_G: 5.8500 D(x): 0.9728 D(G(z)): 0.0907 / 0.0072\n",
      "[0/5][276/3125] Loss_D: 0.1598 Loss_G: 7.7459 D(x): 0.9940 D(G(z)): 0.1319 / 0.0009\n",
      "[0/5][277/3125] Loss_D: 0.0661 Loss_G: 7.2407 D(x): 0.9604 D(G(z)): 0.0224 / 0.0016\n",
      "[0/5][278/3125] Loss_D: 0.1749 Loss_G: 5.9600 D(x): 0.9214 D(G(z)): 0.0788 / 0.0038\n",
      "[0/5][279/3125] Loss_D: 0.5704 Loss_G: 8.9692 D(x): 0.8956 D(G(z)): 0.1937 / 0.0002\n",
      "[0/5][280/3125] Loss_D: 0.1245 Loss_G: 8.4001 D(x): 0.8941 D(G(z)): 0.0025 / 0.0005\n",
      "[0/5][281/3125] Loss_D: 0.4330 Loss_G: 3.9225 D(x): 0.7375 D(G(z)): 0.0220 / 0.0466\n",
      "[0/5][282/3125] Loss_D: 0.6165 Loss_G: 13.0995 D(x): 0.9920 D(G(z)): 0.3917 / 0.0000\n",
      "[0/5][283/3125] Loss_D: 0.6136 Loss_G: 11.4780 D(x): 0.6405 D(G(z)): 0.0001 / 0.0000\n",
      "[0/5][284/3125] Loss_D: 0.8687 Loss_G: 4.0237 D(x): 0.5648 D(G(z)): 0.0002 / 0.0503\n",
      "[0/5][285/3125] Loss_D: 0.7770 Loss_G: 12.1865 D(x): 0.9971 D(G(z)): 0.3962 / 0.0000\n",
      "[0/5][286/3125] Loss_D: 0.1025 Loss_G: 11.5169 D(x): 0.9235 D(G(z)): 0.0006 / 0.0000\n",
      "[0/5][287/3125] Loss_D: 0.0217 Loss_G: 7.8561 D(x): 0.9806 D(G(z)): 0.0013 / 0.0007\n",
      "[0/5][288/3125] Loss_D: 0.0856 Loss_G: 5.6517 D(x): 0.9820 D(G(z)): 0.0599 / 0.0041\n",
      "[0/5][289/3125] Loss_D: 0.4889 Loss_G: 12.2675 D(x): 0.9579 D(G(z)): 0.3141 / 0.0000\n",
      "[0/5][290/3125] Loss_D: 3.1519 Loss_G: 3.8628 D(x): 0.1767 D(G(z)): 0.0002 / 0.0374\n",
      "[0/5][291/3125] Loss_D: 0.8287 Loss_G: 10.0789 D(x): 0.9935 D(G(z)): 0.4312 / 0.0001\n",
      "[0/5][292/3125] Loss_D: 0.1871 Loss_G: 9.9703 D(x): 0.8618 D(G(z)): 0.0012 / 0.0004\n",
      "[0/5][293/3125] Loss_D: 0.3730 Loss_G: 6.4773 D(x): 0.7374 D(G(z)): 0.0014 / 0.0220\n",
      "[0/5][294/3125] Loss_D: 0.1575 Loss_G: 4.2031 D(x): 0.9578 D(G(z)): 0.0806 / 0.0406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0/5][295/3125] Loss_D: 1.1523 Loss_G: 10.6312 D(x): 0.9912 D(G(z)): 0.4620 / 0.0001\n",
      "[0/5][296/3125] Loss_D: 0.2619 Loss_G: 10.8034 D(x): 0.8403 D(G(z)): 0.0004 / 0.0001\n",
      "[0/5][297/3125] Loss_D: 0.4332 Loss_G: 7.0878 D(x): 0.7789 D(G(z)): 0.0045 / 0.0037\n",
      "[0/5][298/3125] Loss_D: 0.2026 Loss_G: 4.8087 D(x): 0.9475 D(G(z)): 0.1160 / 0.0137\n",
      "[0/5][299/3125] Loss_D: 0.4772 Loss_G: 8.0874 D(x): 0.9889 D(G(z)): 0.3177 / 0.0004\n",
      "[0/5][300/3125] Loss_D: 0.4312 Loss_G: 6.0076 D(x): 0.7570 D(G(z)): 0.0166 / 0.0036\n",
      "[0/5][301/3125] Loss_D: 0.4708 Loss_G: 2.9767 D(x): 0.7155 D(G(z)): 0.0565 / 0.0579\n",
      "[0/5][302/3125] Loss_D: 0.9399 Loss_G: 9.2141 D(x): 0.9606 D(G(z)): 0.5188 / 0.0004\n",
      "[0/5][303/3125] Loss_D: 0.1193 Loss_G: 9.1784 D(x): 0.9012 D(G(z)): 0.0013 / 0.0003\n",
      "[0/5][304/3125] Loss_D: 0.0517 Loss_G: 6.7395 D(x): 0.9604 D(G(z)): 0.0100 / 0.0046\n",
      "[0/5][305/3125] Loss_D: 0.5165 Loss_G: 4.6793 D(x): 0.8148 D(G(z)): 0.1458 / 0.0163\n",
      "[0/5][306/3125] Loss_D: 0.4493 Loss_G: 7.1408 D(x): 0.8728 D(G(z)): 0.2432 / 0.0012\n",
      "[0/5][307/3125] Loss_D: 0.3740 Loss_G: 4.8704 D(x): 0.7786 D(G(z)): 0.0530 / 0.0137\n"
     ]
    }
   ],
   "source": [
    "# for each epoch\n",
    "for epoch in range(niter):\n",
    "    # for each minibatch\n",
    "    for i, data in enumerate(cifar_dataloader):\n",
    "        #######\n",
    "        # 1. First we update the D network:\n",
    "        # maximize objective = log(D(x)) + log(1 - D(G(Z)))\n",
    "        # when data comes from real (x) maximize likelihood of guessing real\n",
    "        # D() predicts p_real, so log(D) is log-likelihood of real, so maximize that for x inpout\n",
    "        # 1-D is p_fake, so maximize log-likihood of p_fake for input of fake (D(G(Z)))\n",
    "        # log(1-D(G(Z)))\n",
    "        \n",
    "        #    \n",
    "        #    a. first we get gradients with real \n",
    "        #       so take gradient of log(D(x)) wrt to weights of D \n",
    "        #       then we just store the gradients, but we don't update\n",
    "        #       network yet (we will accumulate gradients)\n",
    "        #       remember d(mean(L_real(data_real)) + mean(L_fake(data_fake)))/dw \n",
    "                # is same as d(mean(L_real(data_real))/dw + d(mean(L_fake(data_fake)))/dw)\n",
    "        \n",
    "        #real\n",
    "        \n",
    "        netD.zero_grad() # zero out gradient buffers\n",
    "        real_cpu, _ = data # we don't need label, real_cpu loads data to cpu\n",
    "        batch_size = real_cpu.size(0)\n",
    "        \n",
    "        if cuda:\n",
    "            real_cpu = real_cpu.cuda() # transfer batch to gpu\n",
    "        \n",
    "        # now we copy real data to the input torch tensor\n",
    "        input.resize_as_(real_cpu).copy_(real_cpu)\n",
    "        \n",
    "        # and we fill the label tensor with the real_label (1), fill it with ones\n",
    "        label.resize_(batch_size).fill_(real_label)\n",
    "        \n",
    "        # now we make Variables for input and label because\n",
    "        # we need to use autograd -> in order to be able to have\n",
    "        # grad functions we need to do operations on Variables\n",
    "        # so the inputs to our graph are variables and the Variable\n",
    "        # constructor needs a torch tensor\n",
    "        inputv = Variable(input)\n",
    "        labelv = Variable(label)\n",
    "        \n",
    "        # output of dsicriminator\n",
    "        output = netD(inputv)\n",
    "        \n",
    "        \n",
    "        # computes average binary cross entropy across all examples\n",
    "        # basically loss of discriminator for the real case \n",
    "        # (aka the first term of gan equation: log(D(x)) b/c labels always 1\n",
    "        # so 1*log(D(x)) + 0 * log(1-D)\n",
    "        errD_real = criterion(output, labelv)\n",
    "        \n",
    "        #now accumulate gradient by calling backward (dont update weights)\n",
    "        # now it will calc gradients all the way back thru the input to the discrim\n",
    "        # b/c forward pass went inputv -> errD\n",
    "        errD_real.backward()\n",
    "        \n",
    "        #average prediction from D for real images\n",
    "        D_x = output.data.mean()\n",
    "             \n",
    "        # now we train with fake!\n",
    "        # resize noise vector and fill it with normal\n",
    "        noise.resize_(batch_size, nz, 1, 1).normal_(0,1)\n",
    "        \n",
    "        # make variable so we can autograd\n",
    "        noisev = Variable(noise)\n",
    "        \n",
    "        #get output from generator\n",
    "        fake = netG(noisev)\n",
    "        \n",
    "        # fill label variable with 0's\n",
    "        labelv = Variable(label.fill_(fake_label))\n",
    "        \n",
    "        # now we get output from discrim when we input fake data\n",
    "        # **** key point here!!! \n",
    "        # (we call detach so fake looks like static input and not just like\n",
    "        # it comes operations on noise!)\n",
    "        # this is because we only want to take gradients wrt to Discrim NOT Generator\n",
    "        # basically takes output of fake and wraps it in a fresh Variable\n",
    "        # so it has no history\n",
    "        output = netD(fake.detach())\n",
    "        \n",
    "        # now get BCE loss for all ground truth of 0, so\n",
    "        # 0*log(output) + 1*log(1-output), which is\n",
    "        # 0* log(D(G(z))) + 1*log(1-D(G(z))) = log(1-D(G(z)))\n",
    "        # which is second term of discrim objective\n",
    "        errD_fake = criterion(output, labelv)\n",
    "        \n",
    "        # accumulate gradients for take loss\n",
    "        errD_fake.backward()\n",
    "        \n",
    "        # average output from discrim for fake images\n",
    "        D_G_z1 = output.data.mean()\n",
    "        \n",
    "        # total loss for discrim for this epoch\n",
    "        errD = errD_real + errD_fake\n",
    "        \n",
    "        # NOW we finally do a weight update!\n",
    "        # we use optimizer D, which updates just the parameters from D\n",
    "        # using the gradient of the parameters\n",
    "        optimizerD.step()\n",
    "        \n",
    "        ###########\n",
    "        # (2) ok now we update the G network: maximize log(D(G(z)))\n",
    "        # aka make the Discriminator think real images came from G\n",
    "        # aka make it think fake images were real\n",
    "        ###########\n",
    "        \n",
    "        # zero out gradient buffers for generator network\n",
    "        netG.zero_grad()\n",
    "        \n",
    "        # fill the label with 1's for real \n",
    "        # (basically fake labels are real for generator cost)\n",
    "        labelv = Variable(label.fill_(real_label))\n",
    "        \n",
    "        # get ouput from discrim when we put in take\n",
    "        # note we use the same fake b/c generator has not changed\n",
    "        # but this output diff then above b/c weights have been changed\n",
    "        # in netG\n",
    "        output = netD(fake)\n",
    "        \n",
    "        # now we take the loss with ground truth as real\n",
    "        # so same as 1a. when we train with real:\n",
    "        # 1*log(D(x)) 0*log(1-D(x)) = log(D(x)) -> first term of objective\n",
    "        # but except instead of D(x) it is D(G(z))\n",
    "        # so the loss is now log(D(G(z)))\n",
    "        # aka log-likelihood of guessing real for fake images\n",
    "        # and we want to mimize negative log likelihood\n",
    "        # so that G changes in a way that would make \n",
    "        # maximize the likelihood of discrim guessing real for fake\n",
    "        errG = criterion(output, labelv)\n",
    "        \n",
    "        \n",
    "        D_G_z2 = output.data.mean()\n",
    "            \n",
    "        # ok so this accumulates gradients\n",
    "        # now note that this wil accumulate\n",
    "        # gradients for D and G cuz it backprops all the way\n",
    "        # thru (we don't detach fake, so we go from noise thru G and then thru D and then the loss)\n",
    "        errG.backward()\n",
    "        \n",
    "        # but we use optimizerG, so we only update weights from G\n",
    "        # and then we will zero out the D gradients\n",
    "        # before we update D, so it wont matter\n",
    "        optimizerG.step()\n",
    "        \n",
    "        \n",
    "        # print out shit\n",
    "        print('[%d/%d][%d/%d] Loss_D: %.4f Loss_G: %.4f D(x): %.4f D(G(z)): %.4f / %.4f'\n",
    "              % (epoch, niter, i, len(cifar_dataloader),\n",
    "                 errD.data[0], errG.data[0], D_x, D_G_z1, D_G_z2))\n",
    "        if i % 100 == 0:\n",
    "            vutils.save_image(real_cpu,\n",
    "                    '%s/real_samples.png' % outf,\n",
    "                    normalize=True)\n",
    "            fake = netG(fixed_noise)\n",
    "            vutils.save_image(fake.data,\n",
    "                    '%s/fake_samples_epoch_%03d.png' % (outf, epoch),\n",
    "                    normalize=True)\n",
    "    #checkpoint every epoch\n",
    "    # state dict saves dict of all weights and buffers of gradients??\n",
    "    torch.save(netG.state_dict(), '%s/netG_epoch_%d.pth' % (outf, epoch))\n",
    "    torch.save(netD.state_dict(), '%s/netD_epoch_%d.pth' % (outf, epoch))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
